# 도커(Docker) 및 스파크(Spark) 심화 가이드

이 문서는 제공된 자료들을 바탕으로 도커의 기본 개념부터 스파크 아키텍처, 그리고 실제 활용법까지 상세하게 설명하고 정리한 종합 가이드입니다.

---

### **1. 도커(Docker) 심층 분석**

도커는 애플리케이션과 그 종속성을 **컨테이너**라는 격리된 공간에 패키징하여, 어떤 환경에서든 일관되게 실행할 수 있도록 하는 플랫폼입니다.

#### **1.1. 도커의 핵심 아키텍처**

*   **이미지(Image)와 레이어(Layer)**
    *   **이미지**: 애플리케이션 실행에 필요한 모든 것(코드, 런타임, 라이브러리 등)을 담은 **읽기 전용(Read-Only) 템플릿**입니다.
    *   **레이어 시스템**: 이미지는 여러 개의 얇은 레이어로 구성됩니다. 베이스 이미지(예: Ubuntu) 위에 패키지 설치, 소스코드 복사 등의 변경사항이 순차적으로 레이어로 쌓입니다. 소스코드가 변경되면 해당 레이어만 교체하여 빌드하므로 매우 효율적입니다.
*   **컨테이너(Container)**
    *   이미지를 실행한 **인스턴스**입니다.
    *   기존 이미지 레이어 위에 **읽기/쓰기(Read/Write)가 가능한 '컨테이너 레이어'**가 추가됩니다. 애플리케이션이 실행되면서 생성하는 로그, 캐시 등은 이 레이어에 저장됩니다.
    *   컨테이너는 삭제 시 이 컨테이너 레이어도 함께 사라지므로, 데이터를 영구적으로 보존하려면 **볼륨(Volume)**을 사용해야 합니다.
*   **레지스트리(Registry)**
    *   이미지를 저장하고 공유하는 중앙 저장소입니다. **도커 허브(Docker Hub)**가 가장 대표적인 공개 레지스트리이며, 보안이 중요한 경우 기업 내부에 비공개 레지스트리를 구축하여 사용합니다.
*   **도커 엔진(Engine) & 클라이언트(Client)**
    *   **엔진(데몬)**: 도커 호스트에 설치되어 이미지와 컨테이너를 생성하고 관리하는 핵심 백그라운드 프로세스입니다.
    *   **클라이언트**: 사용자가 `docker run`, `docker build` 같은 명령어를 입력하는 인터페이스입니다. 이 명령어는 도커 엔진에 전달되어 실제 작업을 수행합니다.

#### **1.2. 도커의 장점과 활용**

*   **환경 일관성 및 빠른 배포**: 개발, 테스트, 운영 환경을 완벽히 통일하고, 가상 머신(VM)보다 훨씬 가볍고 빠르게 애플리케이션을 배포하고 확장할 수 있습니다.
*   **리소스 효율성**: 호스트 OS의 커널을 공유하므로 VM보다 월등히 적은 자원으로 더 많은 애플리케이션을 실행할 수 있습니다.
*   **마이크로서비스 및 CI/CD**: 각 서비스를 독립적인 컨테이너로 분리하여 개발하고, GitHub에 코드를 푸시하면 CI/CD 파이프라인이 자동으로 이미지를 빌드하고 배포하는 자동화 구축에 핵심적인 역할을 합니다.
*   **데이터 파이프라인 구축**: Kafka(수집), Spark(처리), MySQL(저장), Grafana(시각화) 등 데이터 처리의 각 단계를 독립적인 컨테이너로 구성하여 유연하고 확장성 있는 파이프라인을 만들 수 있습니다.
*   **클라우드 연동**: AWS의 ECS, EKS, Fargate와 같은 주요 클라우드 서비스들은 도커 컨테이너를 손쉽게 배포하고 관리할 수 있는 강력한 플랫폼을 제공합니다.

---

### **2. 웹 서비스 아키텍처의 이해**

현대 웹 서비스는 역할에 따라 여러 계층으로 나뉩니다(N-tier Architecture).

*   **웹 서버 (Web Server)**: 정적 콘텐츠(HTML, CSS, 이미지)를 제공합니다. (예: Nginx, Apache)
*   **웹 애플리케이션 서버 (WAS)**: 동적 콘텐츠를 생성합니다. 사용자의 요청에 따라 비즈니스 로직을 실행하고 데이터베이스와 상호작용하여 결과를 웹 서버에 전달합니다. (예: Tomcat, Gunicorn)
*   **데이터베이스 (Database)**: 데이터를 영구적으로 저장하고 관리합니다. (예: MySQL, PostgreSQL)

이러한 각 구성 요소를 개별 도커 컨테이너로 실행하면, 서비스 간의 의존성을 낮추고 독립적으로 확장 및 업데이트할 수 있어 유지보수성이 크게 향상됩니다.

---

### **3. 도커 실전 활용: MySQL & PySpark 환경 구축**

#### **3.1. 도커로 MySQL 서버 구축하기**

`docker run` 명령어 하나로 MySQL 서버를 즉시 실행할 수 있습니다.

```bash
# -d: 백그라운드에서 컨테이너 실행
# -p 3307:3306: 호스트의 3307 포트를 컨테이너의 3306 포트로 포워딩
# -e MYSQL_ROOT_PASSWORD=1234: 컨테이너 내 환경 변수 설정 (MySQL 루트 비밀번호)
# -v /home/user/mysql_data:/var/lib/mysql: 호스트의 폴더를 컨테이너의 폴더에 연결(마운트)
docker run -d -p 3307:3306 -e MYSQL_ROOT_PASSWORD=1234 -v /home/user/mysql_data:/var/lib/mysql mysql
```

*   **데이터 영속성을 위한 볼륨(`-v`)**: 이 옵션이 **가장 중요**합니다. 컨테이너는 일회성이므로 삭제하면 데이터가 모두 사라집니다. `-v` 옵션으로 호스트의 특정 폴더(좌측 경로)를 컨테이너의 데이터 저장 폴더(우측 경로, MySQL의 경우 `/var/lib/mysql`)와 연결하면, 데이터가 호스트 컴퓨터에 저장됩니다. 따라서 컨테이너를 삭제하고 새로 생성해도 데이터는 그대로 유지됩니다.

#### **3.2. Docker Compose로 PySpark 개발 환경 구성하기**

`Dockerfile`과 `docker-compose.yml`을 사용하면 복잡한 개발 환경을 코드로 명세하고, 명령어 한 줄로 관리할 수 있습니다.

*   **`Dockerfile`**: 나만의 커스텀 이미지를 만들기 위한 설계도입니다.
    ```dockerfile
    # 공식 PySpark + Jupyter 이미지를 기반으로 시작
    FROM jupyter/pyspark-notebook
    # root 권한으로 전환하여 패키지 설치
    USER root
    RUN apt-get update && apt-get install -y vim tree net-tools
    # 다시 기본 사용자로 전환
    USER jovyan
    ```
*   **`docker-compose.yml`**: 여러 컨테이너의 설정과 실행을 정의하는 파일입니다.
    ```yaml
    version: '3'
    services:
      spark:
        build: .  # 현재 디렉토리의 Dockerfile을 사용해 이미지 빌드
        container_name: pyspark-notebook
        ports:
          - "8888:8888"  # Jupyter Notebook 접속 포트
          - "4040:4040"  # Spark UI 접속 포트
        volumes:
          # 로컬의 workspace 폴더를 컨테이너의 작업 폴더와 연결하여 코드 공유
          - ./workspace:/home/jovyan/work
    ```
    이후 터미널에서 `docker compose up --build -d` 명령어를 실행하면, `Dockerfile`로 이미지를 빌드하고 `docker-compose.yml`에 정의된 설정대로 컨테이너를 실행합니다.

---

### **4. 아파치 스파크(Apache Spark) 심층 분석**

스파크는 하둡 맵리듀스의 디스크 기반 처리 방식의 한계를 **인메모리(In-Memory) 컴퓨팅**으로 극복한 고성능 분산 처리 엔진입니다.

#### **4.1. 스파크 아키텍처와 구성 요소**

스파크는 마스터-워커 구조의 클러스터에서 동작합니다.

*   **드라이버 (Driver)**: 사용자의 메인 프로그램을 실행하며, `SparkContext`를 생성하고 전체 작업의 흐름(DAG)을 만들고 스케줄링하는 컨트롤 타워입니다.
*   **클러스터 매니저 (Cluster Manager)**: 클러스터의 자원을 관리하고 드라이버의 요청에 따라 익스큐터를 할당합니다. (종류: Standalone, YARN, Mesos)
*   **익스큐터 (Executor)**: 워커 노드(Worker Node)에서 실행되는 프로세스로, 드라이버로부터 할당받은 실제 계산(태스크)을 수행하고 결과를 반환합니다.

#### **4.2. 스파크의 핵심 동작 원리**

*   **RDD (Resilient Distributed Dataset)**: 스파크의 핵심 데이터 모델입니다. 여러 노드에 분산된, **변경 불가능(Immutable)**하며, 일부 노드에 장애가 발생해도 **자동으로 복구(Resilient)**되는 데이터 집합입니다. RDD의 모든 변환 과정은 **계보(Lineage)** 정보로 기록되어 장애 발생 시 이를 통해 데이터를 복원합니다.
*   **DAG (Directed Acyclic Graph)**: 스파크의 실행 계획입니다. RDD에 적용되는 일련의 변환(Transformation)들을 방향성이 있는 비순환 그래프로 만듭니다.
*   **지연 평가 (Lazy Evaluation)**: `map`, `filter` 같은 변환은 즉시 실행되지 않고 DAG에 계획으로만 기록됩니다. `collect`, `count` 같은 **액션(Action)**이 호출되어야 비로소 스파크는 전체 DAG를 최적화하여 실제 연산을 시작합니다.

#### **4.3. 스파크 작업 실행 흐름**

1.  **코드 제출**: 사용자가 작성한 스파크 애플리케이션이 드라이버에 제출됩니다.
2.  **DAG 생성**: 드라이버는 코드를 분석하여 RDD 변환 과정을 **DAG**로 만듭니다.
3.  **스테이지 분할**: DAG는 셔플(Shuffle, 노드 간 데이터 재분배)이 필요한 지점을 기준으로 여러 **스테이지(Stage)**로 나뉩니다.
4.  **태스크 생성**: 각 스테이지는 병렬로 실행될 수 있는 최소 작업 단위인 **태스크(Task)**들로 쪼개집니다. 태스크 하나는 보통 파티션 하나에 대한 연산을 의미합니다.
5.  **태스크 실행**: 드라이버는 이 태스크들을 클러스터 매니저를 통해 각 워커 노드의 익스큐터에 분배하여 실행시킵니다.

#### **4.4. Pandas vs. Spark**

*   **Pandas**: 단일 머신에서 동작하며 메모리 크기에 의존적입니다. 데이터가 메모리보다 커지면 **메모리 부족(Out of Memory)** 오류가 발생합니다.
*   **Spark**: 여러 머신에 데이터를 분산하여 처리하므로 **메모리보다 훨씬 큰 대용량 데이터**도 안정적으로 처리할 수 있습니다. 분산 환경 설정으로 인한 오버헤드 때문에 작은 데이터에서는 Pandas보다 느릴 수 있지만, 데이터 규모가 커질수록 압도적인 성능을 보입니다.

---
---
**2025년 8월 5일 추가된 내용**
### **5. 스파크 실전 예제: 데이터 처리 및 외부 연동**

제공된 예제 코드들은 스파크를 활용한 데이터 처리, 분석 및 외부 데이터베이스 연동의 구체적인 방법을 보여줍니다.

#### **5.1. 데이터 로드 및 기본 처리 (`02_mnms_result_load.py`)**

*   스파크 세션을 초기화하고, `sparkContext.textFile()`을 사용하여 HDFS나 로컬 파일 시스템에 저장된 텍스트 기반의 결과 데이터를 RDD로 로드합니다.
*   `take()`, `count()`와 같은 기본 RDD 액션을 사용하여 데이터를 확인하고 간단한 통계를 계산합니다.

#### **5.2. Spark SQL과 데이터프레임 (`05_SparkSQL_DF.ipynb`)**

*   **RDD에서 데이터프레임으로**: RDD를 사용한 조인 및 필터링 방식과 데이터프레임을 사용한 방식을 비교하여 보여줍니다.
*   **데이터프레임 생성 및 조작**: 스키마와 함께 `createDataFrame()`으로 데이터프레임을 직접 생성하고, `select`, `filter`, `show` 등 다양한 API를 활용하여 데이터를 조작합니다.
*   **SQL 쿼리 실행**: `createOrReplaceTempView()`를 통해 데이터프레임을 SQL에서 사용할 수 있는 임시 테이블(뷰)로 등록한 후, 표준 SQL 구문(`SELECT`, `WHERE`, `LIKE`, `BETWEEN` 등)으로 데이터를 조회하고 분석합니다. 이는 데이터 분석가들에게 매우 친숙한 인터페이스를 제공합니다.
*   **스키마 정의**: `StructType`과 `StructField`를 사용하여 데이터프레임의 스키마를 프로그래매틱하게, 그리고 명시적으로 정의하는 방법을 다룹니다.
*   **집계 및 조인**: `agg`, `groupBy`, `join`과 같은 강력한 API를 사용하여 데이터를 그룹화하고, 집계(평균, 개수 등)하며, 여러 데이터프레임을 결합하는 방법을 보여줍니다.
*   **쿼리 최적화**: `explain()` 함수를 통해 스파크의 Catalyst 옵티마이저가 SQL 쿼리를 어떻게 분석하고 물리적 실행 계획을 수립하는지 확인할 수 있습니다. 이를 통해 쿼리 성능 튜닝의 기초를 이해할 수 있습니다.

#### **5.3. 데이터 분석 심화 (`06_SparkDataAnal.ipynb`)**

*   **다양한 데이터 소스**: `spark.read.format('json')`을 사용하여 JSON 파일을 데이터프레임으로 손쉽게 로드합니다.
*   **데이터 탐색**: `printSchema()`, `dtypes`, `distinct()` 등의 함수로 데이터의 구조와 내용을 탐색합니다.
*   **동적 컬럼 추가 및 변형**: `withColumn()`과 SQL 표현식을 사용하는 `expr()` 함수를 결합하여 기존 컬럼을 바탕으로 새로운 컬럼(예: `withinCountry`)을 동적으로 추가합니다.
*   **조건부 로직**: `CASE WHEN` 구문을 `expr()` 내에서 사용하여 데이터 값에 따라 새로운 카테고리(예: `upper`, `under`)를 부여하는 등 복잡한 비즈니스 로직을 구현합니다.

#### **5.4. 외부 데이터베이스 연동: MySQL (`07_mysql_connect_test.ipynb`, `untitled.py`)**

*   **JDBC를 이용한 데이터 읽기/쓰기**: 스파크는 `format("jdbc")`를 통해 표준 JDBC 인터페이스를 지원하는 모든 관계형 데이터베이스(RDBMS)와 연동할 수 있습니다.
*   **연결 설정**: `url`, `driver`, `dbtable`, `user`, `password` 등 필수 옵션을 설정하여 MySQL에 연결합니다.
    *   **네트워크 주의사항**: 도커 컨테이너 내부에서 다른 컨테이너의 MySQL에 접근할 때는 서비스 이름(예: `mysql:3306`)을 사용하고, 외부에서 접근할 때는 포트 포워딩된 IP와 포트(예: `192.168.47.132:3307`)를 사용해야 합니다.
*   **데이터 로드 및 저장**:
    *   `spark.read.jdbc(...)`를 사용하여 테이블 전체 또는 특정 쿼리 결과를 데이터프레임으로 가져올 수 있습니다.
    *   `df.write.jdbc(...)`를 사용하여 스파크에서 처리한 결과 데이터프레임을 MySQL 테이블에 저장(`append`, `overwrite` 모드 등)할 수 있습니다. 이는 데이터 파이프라인의 최종 결과를 저장하는 일반적인 패턴입니다.
