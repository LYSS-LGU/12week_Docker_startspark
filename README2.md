# 도커(Docker) 및 스파크(Spark) 핵심 개념 요약

이 문서는 제공된 자료들을 바탕으로 도커의 기본 개념부터 스파크 아키텍처까지의 핵심 내용을 종합적으로 요약 및 정리한 것입니다.

---

### **1. 도커(Docker)의 핵심 개념**

도커는 애플리케이션을 신속하게 구축, 테스트 및 배포할 수 있는 컨테이너화 플랫폼입니다. "한 번 빌드하면, 어디서든 실행된다"는 철학을 바탕으로 환경의 일관성을 보장합니다.

#### **1.1. 주요 구성 요소**

*   **이미지 (Image)**: 애플리케이션 실행에 필요한 모든 것(코드, 런타임, 라이브러리, 환경 변수, 설정 파일)을 포함하는 **읽기 전용(Read-Only) 템플릿**입니다. 여러 개의 레이어(Layer)로 구성되어 효율적인 빌드와 배포가 가능합니다.
*   **컨테이너 (Container)**: 이미지의 실행 가능한 인스턴스입니다. 이미지 레이어 위에 **읽기/쓰기(Read/Write)가 가능한 컨테이너 레이어**가 추가되어, 애플리케이션이 실행되면서 발생하는 변경사항(로그, 캐시 등)을 저장합니다. 컨테이너는 격리된 환경에서 독립적으로 실행됩니다.
*   **레지스트리 (Registry)**: 도커 이미지를 저장하고 공유하는 중앙 저장소입니다. 대표적으로 **도커 허브(Docker Hub)**가 있으며, 비공개(Private) 레지스트리를 구축하여 사용할 수도 있습니다.

#### **1.2. 도커의 장점**

*   **환경 일관성**: 개발, 테스트, 운영 환경을 동일하게 유지하여 "제 컴퓨터에서는 됐는데..." 하는 문제를 해결합니다.
*   **빠른 배포 및 확장성**: 가상 머신(VM)보다 훨씬 가볍고 빠르게 시작되며, 필요에 따라 수평적으로 쉽게 확장할 수 있습니다.
*   **리소스 효율성**: 호스트 OS의 커널을 공유하므로 VM에 비해 시스템 자원을 훨씬 효율적으로 사용합니다.
*   **마이크로서비스 아키텍처**: 각 서비스를 독립적인 컨테이너로 분리하여 개발, 배포, 관리가 용이한 마이크로서비스 구축에 이상적입니다.

---

### **2. 웹 서비스 아키텍처와 도커의 역할**

일반적인 웹 서비스는 사용자의 요청을 받는 **웹 서버(Web Server)**, 비즈니스 로직을 처리하는 **웹 애플리케이션 서버(WAS)**, 데이터를 저장하는 **데이터베이스(DB)**로 구성됩니다.

도커는 이러한 각 구성 요소를 개별 컨테이너로 만들어 독립적으로 관리할 수 있게 해줍니다. 예를 들어, Nginx 웹 서버 컨테이너, Tomcat WAS 컨테이너, MySQL DB 컨테이너를 각각 실행하여 유연하고 확장 가능한 시스템을 구축할 수 있습니다.

---

### **3. 도커 실전 활용 예시**

#### **3.1. MySQL 서버 구축하기**

`docker run` 명령어를 사용하여 간단하게 MySQL 서버를 실행할 수 있습니다.

```docker
# -d: 백그라운드 실행
# -p 3307:3306: 호스트의 3307 포트를 컨테이너의 3306 포트와 연결
# -e: 환경 변수 설정 (MySQL 루트 비밀번호)
# -v: 볼륨 마운트 (데이터 영속성을 위해)
docker run -d -p 3307:3306 \
  -e MYSQL_ROOT_PASSWORD=1234 \
  -v /home/user/mysql_data:/var/lib/mysql \
  mysql
```

*   **핵심 개념: 볼륨(Volume)**
    *   컨테이너는 삭제 시 내부 데이터가 함께 사라집니다.
    *   데이터의 **영속성(Persistence)**을 보장하기 위해, 호스트 컴퓨터의 특정 폴더(`-v` 옵션으로 지정)를 컨테이너 내부의 데이터 저장 경로(`/var/lib/mysql`)와 연결합니다.
    *   이렇게 하면 컨테이너를 삭제하고 새로 만들어도 데이터는 호스트에 그대로 남아있게 됩니다.

#### **3.2. PySpark 개발 환경 구성하기**

`Dockerfile`과 `docker-compose.yml`을 사용하면 복잡한 개발 환경을 코드로 관리하고 쉽게 구축할 수 있습니다.

*   **Dockerfile**: 베이스 이미지를 바탕으로 필요한 패키지 설치 등 커스텀 이미지를 만들기 위한 "레시피"입니다.
    ```dockerfile
    # jupyter/pyspark-notebook 이미지를 기반으로 함
    FROM jupyter/pyspark-notebook
    USER root
    # vim, tree 등 추가 도구 설치
    RUN apt-get update && apt-get install -y vim tree
    USER jovyan
    ```
*   **docker-compose.yml**: 여러 컨테이너를 정의하고 한 번에 실행, 중지할 수 있게 해주는 "오케스트레이션 도구"입니다.
    ```yaml
    version: '3'
    services:
      spark:
        build: .  # 현재 디렉토리의 Dockerfile을 사용해 이미지 빌드
        container_name: pyspark-notebook
        ports:
          - "8888:8888"  # Jupyter Notebook 포트
          - "4040:4040"  # Spark UI 포트
        volumes:
          # 로컬의 workspace 폴더를 컨테이너의 작업 폴더와 연결
          - ./workspace:/home/jovyan/work
    ```
    `docker compose up --build -d` 명령어로 모든 설정이 적용된 컨테이너를 빌드하고 실행할 수 있습니다.

---

### **4. 아파치 스파크(Apache Spark) 아키텍처**

스파크는 대용량 데이터 처리를 위한 빠르고 범용적인 **통합 분석 엔진**입니다. 하둡 맵리듀스의 느린 디스크 I/O 문제를 **인메모리(In-Memory) 컴퓨팅**으로 해결하여 성능을 극대화했습니다.

#### **4.1. 스파크 아키텍처**

스파크는 **클러스터** 환경에서 동작하며, 마스터-워커 구조를 가집니다.

*   **드라이버 (Driver)**: 사용자의 코드를 받아 전체 작업 흐름(DAG)을 생성하고, 클러스터에 작업을 할당하며 실행을 총괄합니다.
*   **클러스터 매니저 (Cluster Manager)**: 클러스터의 자원을 관리합니다. (Standalone, YARN, Mesos 등)
*   **워커 노드 (Worker Node)**: 실제 계산을 수행하는 **익스큐터(Executor)** 프로세스를 실행합니다.

#### **4.2. 핵심 추상화 개념**

*   **RDD (Resilient Distributed Dataset)**: 스파크의 핵심 데이터 구조입니다. 여러 노드에 분산된, **변경 불가능(Immutable)**하며, 장애 시 **자동으로 복구(Resilient)**되는 데이터 컬렉션입니다.
*   **DAG (Directed Acyclic Graph)**: 스파크의 작업 실행 계획입니다. RDD에 적용되는 일련의 변환(Transformation)들을 방향성이 있는 비순환 그래프로 표현합니다. 스파크는 이 DAG를 최적화하여 가장 효율적인 순서로 작업을 실행합니다.

#### **4.3. 스파크 작업 흐름**

1.  사용자 코드가 드라이버에 제출됩니다.
2.  드라이버는 코드를 분석하여 논리적인 실행 계획인 **DAG**를 생성합니다.
3.  DAG는 셔플(Shuffle)을 기준으로 여러 **스테이지(Stage)**로 나뉩니다.
4.  각 스테이지는 병렬로 실행될 수 있는 작은 작업 단위인 **태스크(Task)**들로 구성됩니다.
5.  드라이버는 이 태스크들을 클러스터 매니저를 통해 각 워커 노드의 익스큐터에 할당하여 실행합니다.
6.  **지연 평가(Lazy Evaluation)** 덕분에, `collect()`, `count()` 같은 **액션(Action)**이 호출되기 전까지는 어떤 계산도 실제로 일어나지 않으며, 액션이 호출되는 순간 전체 DAG가 최적화되어 실행됩니다.
