# 스파크 RDD 심화 요약 정리

---

### **1. RDD란 무엇인가? (핵심 특징)**

**RDD (Resilient Distributed Dataset, 탄력적 분산 데이터셋)** 는 여러 서버에 분산되어 **병렬로 처리**될 수 있는, **장애를 스스로 복구**할 수 있는 데이터 구조입니다.

*   **분산 및 병렬 처리 (Distributed & Parallel)**: 데이터가 여러 조각(파티션)으로 나뉘어 클러스터에서 병렬로 처리되므로 대용량 데이터 처리에 매우 빠릅니다.
*   **탄력성 (Resilient)**: 작업 중 특정 서버(노드)에 문제가 생겨도, 다른 노드에 복제된 데이터를 이용해 **자동으로 작업을 복구**합니다.
*   **불변성 (Immutable)**: RDD는 한 번 생성되면 **변경할 수 없습니다**. 데이터를 수정하려면 기존 RDD를 기반으로 새로운 RDD를 만드는 **변환(Transformation)**을 거쳐야 합니다. 이는 데이터의 일관성을 보장하고 복잡한 연산을 추적하기 쉽게 만듭니다.
*   **지연 연산 (Lazy Evaluation)**: `map`, `filter` 같은 변환 작업은 호출 즉시 실행되지 않습니다. 대신 모든 변환 계획(DAG)을 짜두었다가, 결과를 요청하는 **액션(Action)**이 호출될 때 한꺼번에 실행하여 전체 프로세스를 최적화합니다.

---

### **2. RDD의 기본 작업 흐름**

모든 RDD 작업은 **`생성 → 변환(0회 이상) → 액션(1회)`** 의 명확한 3단계 흐름을 따릅니다.

1.  **생성 (Creation)**: 로컬 컬렉션(`sc.parallelize()`)이나 외부 저장소(`sc.textFile()`)에서 데이터를 읽어 RDD를 만듭니다.
2.  **변환 (Transformation)**: 기존 RDD를 가공해 새로운 RDD를 만듭니다. (예: `map`, `filter`, `reduceByKey`)
3.  **액션 (Action)**: 모든 변환 작업을 실제로 실행시키고, 결과를 반환받거나 저장합니다. (예: `count`, `collect`, `saveAsTextFile`)

---

### **3. 트랜스포메이션 (Transformation): 데이터 가공**

변환은 **셔플(Shuffle)** 발생 여부에 따라 두 종류로 나뉩니다. 셔플은 다른 노드 간 데이터 이동이 발생하는 비싼 작업이므로, 이를 최소화하는 것이 스파크 성능 튜닝의 핵심입니다.

#### **Narrow (좁은) 변환: 셔플이 없는 빠른 연산**

하나의 파티션이 다른 파티션의 데이터 없이 독립적으로 결과를 낼 수 있는 1:1 변환입니다.

*   `map(func)`: 각 요소에 함수를 적용해 새로운 RDD를 만듭니다.
*   `filter(func)`: 조건 함수를 만족하는 요소만 남깁니다.
*   `flatMap(func)`: `map`과 유사하지만, 하나의 입력에서 여러 개의 출력을 만들 수 있습니다. (예: 문장을 단어 리스트로 변환)

#### **Wide (넓은) 변환: 셔플이 발생하는 비싼 연산**

다른 파티션의 데이터를 가져와 섞어야 하는(셔플 발생) 연산입니다. 정렬, 그룹화 등이 해당됩니다.

*   `groupByKey()`: 같은 키를 가진 모든 값을 리스트로 묶습니다. **주의:** 모든 데이터가 드라이버로 모일 수 있어 메모리 문제를 유발할 수 있습니다.
*   `reduceByKey(func)`: **(성능 핵심)** 같은 키를 가진 값들을 `func`로 집계합니다. `groupByKey().map()` 조합보다 훨씬 효율적인데, 셔플 전에 각 노드에서 미리 집계를 수행하여 네트워크로 오가는 데이터 양을 크게 줄이기 때문입니다.
*   `join()`: 두 RDD를 키 기준으로 결합합니다.
*   `sortByKey()`: 키를 기준으로 데이터를 정렬합니다.

---

### **4. 액션 (Action): 결과 도출 및 실행 트리거**

액션은 지연됐던 모든 변환을 실행시키는 "방아쇠" 역할을 합니다.

*   `collect()`: **(주의!)** 모든 데이터를 드라이버(내 컴퓨터) 메모리로 가져옵니다. 데이터가 크면 **메모리 부족(OOM) 에러**가 발생하므로, 테스트나 작은 데이터에만 사용해야 합니다.
*   `take(n)`: `n`개의 데이터만 안전하게 가져옵니다. (`collect`의 안전한 대안)
*   `count()`: 요소의 총 개수를 반환합니다.
*   `reduce(func)`: 모든 요소를 `func`를 사용해 하나의 값으로 합칩니다. (예: 총합 계산)
*   `aggregate(zeroValue, seqOp, combOp)`: `reduce`의 고급 버전. 파티션 내부 연산(`seqOp`)과 파티션 간 연산(`combOp`)을 다르게 지정할 수 있어, 평균 계산처럼 복잡한 집계에 유용합니다.

---

### **5. Key-Value RDD: 그룹화와 집계의 핵심**

`(Key, Value)` 쌍으로 이루어진 RDD로, 스파크의 강력한 집계 연산의 기반이 됩니다.

*   **생성**: 일반 RDD에 `map(lambda x: (key, value))`을 적용하여 만듭니다.
*   **주요 연산**:
    *   `reduceByKey(func)`: **(가장 중요)** 키별로 값을 효율적으로 집계합니다.
    *   `mapValues(func)`: 키는 유지하고 값에만 함수를 적용하여 파티션을 보존하므로 효율적입니다.
    *   `join()`, `leftOuterJoin()`, `rightOuterJoin()`: SQL처럼 두 RDD를 키 기준으로 조인합니다.

#### **대표 예시: 음식 평점 평균 계산**
```python
# (음식명, 평점) 데이터 생성
data = [("짜장면", 4), ("짜장면", 5), ("짬뽕", 3), ("짬뽕", 5)]
rdd = sc.parallelize(data)

# 1. (Key, [Value, 1]) 형태로 변환 -> (음식명, [평점, 주문수])
# 예: ("짜장면", 4) -> ("짜장면", [4, 1])
pair_rdd = rdd.map(lambda x: (x[0], [x[1], 1]))

# 2. reduceByKey로 키별 [평점 합, 주문수 합] 계산
# ("짜장면", [4, 1]), ("짜장면", [5, 1]) -> ("짜장면", [9, 2])
reduced = pair_rdd.reduceByKey(lambda a, b: [a[0] + b[0], a[1] + b[1]])

# 3. mapValues로 최종 평균 계산
# ("짜장면", [9, 2]) -> ("짜장면", 4.5)
avg_rating = reduced.mapValues(lambda x: x[0] / x[1])

# 4. 액션으로 결과 확인
print(avg_rating.collect()) # [('짜장면', 4.5), ('짬뽕', 4.0)]
```